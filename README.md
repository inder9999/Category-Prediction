# Category-Prediction

CNN : CNN’s are one of the widely used networks because of its capability to capture the spatial patterns. In NLP, CNN’s are well able to look for n-grams in the data. We have used word Embeddings as the input to the CNN model, on which a CNN layer is used. To reduce the dimensions and to ensure that only important information is passed to the next dense layers for classification a mapool layer is added. Hyperparameter Tuning was done for the embedding dimension, kernel size, filter numbers, dropout ratio and batch size.

LSTM : In Natural Language, it is the Sequential Arrangement of the words that give meaning to the text. LSTM are one of the most popularly enjoyed networks used in the text classification to capture the sequence of the text. We have used the Bidirectional LSTM (BiLSTM) which gives superiority to the ordinary LSTM by capturing the sequences from backward direction also in addition to the forward direction. The output from the BiLSTM is forwarded to the dense layer for the classification. 

C-LSTM : C-LSTM models take advantage of both the CNN and BiLSTM. A CNN layer followed by the BiLSTM is used here. In this model spatial patterns extracted using the CNN are passed to the BiLSTM, enabling it to capture the sequence on a broader view. 

Very Deep CNN (VDCNN) : In Computer Vision Deeper CNN’s have always been advantageous in classification tasks. Some works using the same have also been done in Classification tasks in NLP. We have implemented the Deeper CNN’s of varying length and have analysed their performance. As the network goes deeper, it becomes harder to train because of the vanishing gradient problem and hence need larger dataset and more training time. A residual connection can help in maintaining the flow of gradients and can make the training of VDCNN easier in deficiency of training data also. 

AttBiLSTM : AttBiLSTM model consists of a Bidirectional LSTM which is followed by the attention mechanism. Here BiLSTM  works like an encoder and return sequence of the same length as that of the input. Attention Mechanism gives attention or weightage to the encoded sequence which is summed and is passed forward to the dense layers for the classification.

HAN : Hierarchical Attention Network (HAN) works in the two Hierarchical phases consisting of an AttBiLSTM in each phase. Here rather than passing the text directly as sequence of words it is passed as sequence of sentences which themselves are sequences of word, In first phase individual sentences are passed through a AttBiLSTM unit giving the encoded form of the sentences which is attention weighted sum of the sequences returned from the BiLSTM. Now the encoded sentences in each text are considered as a sequence and are passed through the second AttBiLSTM unit. At last dense layers are added to the model to classify the input text.

Densely Connected CNN with Multi Scale feature attention : Inspired from the Computer Vision tasks, this method is based on the dense connections and attention weighted multi scale features. Input text is represented as a planar array which is passed through densely connected CNN layers to obtain multi scale high dimensional features. These Multi scale features are pooled down in a manner such that their dimensions become the same. The features are weighted by the attention and are summed before passing through the dense layers for the classification.

ABCDM : Attention-based Bidirectional CNN-RNN Deep Model (ABCDM) is based on the multiple Techniques consisting of , First of all the input text sequence in the form of the word embeddings is passed through the Bidirectional GRU and Bidirectional LSTM in parallel The returned sequences form both are weighted by the attention mechanism and are further passed to multi scale CNNs. The output from multi scale CNNs are downsampled with the help of Max Pooling and Average Pooling. At last pooled results are concatenated and are passed through the  Dense layers for the classification.

